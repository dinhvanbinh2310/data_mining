{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ƒê·ªì √°n Khai th√°c D·ªØ li·ªáu - Main Workflow\n",
        "\n",
        "Notebook n√†y th·ª±c hi·ªán to√†n b·ªô pipeline t·ª´ ti·ªÅn x·ª≠ l√Ω ƒë·∫øn ƒë√°nh gi√° m√¥ h√¨nh.\n",
        "\n",
        "## Quy tr√¨nh:\n",
        "1. Load v√† ki·ªÉm tra dataset\n",
        "2. Th·ªëng k√™ m√¥ t·∫£\n",
        "3. Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu\n",
        "4. Training 2 m√¥ h√¨nh kh√°c lo·∫°i\n",
        "5. ƒê√°nh gi√° v√† so s√°nh m√¥ h√¨nh\n",
        "6. L∆∞u k·∫øt qu·∫£\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add parent directory to path ƒë·ªÉ import c√°c module\n",
        "current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
        "parent_dir = os.path.dirname(current_dir)\n",
        "sys.path.insert(0, parent_dir)\n",
        "\n",
        "from src.utils.descriptive_stats import generate_descriptive_stats, save_stats_to_markdown, save_stats_to_csv\n",
        "from src.preprocess.preprocessing import DataPreprocessor, split_data\n",
        "from src.models.train_model import ModelTrainer\n",
        "from src.evaluation.evaluate import ModelEvaluator\n",
        "\n",
        "# Set style\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8')\n",
        "except:\n",
        "    plt.style.use('seaborn')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"ƒê√£ import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load v√† Ki·ªÉm tra Dataset\n",
        "\n",
        "**L∆∞u √Ω**: ƒê·∫∑t file dataset v√†o `data/raw/` tr∆∞·ªõc khi ch·∫°y\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "# THAY ƒê·ªîI T√äN FILE THEO DATASET C·ª¶A B·∫†N\n",
        "data_path = \"../data/raw/dataset.csv\"  # ƒêi·ªÅu ch·ªânh ƒë∆∞·ªùng d·∫´n v√† t√™n file\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(data_path)\n",
        "    print(f\"‚úÖ ƒê√£ load dataset: {df.shape[0]} d√≤ng, {df.shape[1]} c·ªôt\")\n",
        "    print(f\"\\nC√°c c·ªôt: {list(df.columns)}\")\n",
        "    print(f\"\\n5 d√≤ng ƒë·∫ßu:\")\n",
        "    df.head()\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå Kh√¥ng t√¨m th·∫•y file: {data_path}\")\n",
        "    print(\"Vui l√≤ng ƒë·∫∑t dataset v√†o data/raw/\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Th·ªëng k√™ M√¥ t·∫£\n",
        "\n",
        "T·∫°o th·ªëng k√™ m√¥ t·∫£ t·ª± ƒë·ªông v√† l∆∞u v√†o report/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ki·ªÉm tra dataset h·ª£p l·ªá (>= 5 c·ªôt, >= 500 d√≤ng)\n",
        "if df.shape[1] >= 5 and df.shape[0] >= 500:\n",
        "    print(\"‚úÖ Dataset h·ª£p l·ªá\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è C·∫£nh b√°o: Dataset c√≥ {df.shape[0]} d√≤ng, {df.shape[1]} c·ªôt\")\n",
        "    print(\"Y√™u c·∫ßu: >= 500 d√≤ng v√† >= 5 c·ªôt\")\n",
        "\n",
        "# T·∫°o th·ªëng k√™ m√¥ t·∫£\n",
        "stats = generate_descriptive_stats(df)\n",
        "\n",
        "# L∆∞u th·ªëng k√™\n",
        "save_stats_to_markdown(stats, \"../report/descriptive_stats.md\")\n",
        "save_stats_to_csv(df, \"../src/utils/descriptive_stats.csv\")\n",
        "\n",
        "print(\"‚úÖ ƒê√£ t·∫°o v√† l∆∞u th·ªëng k√™ m√¥ t·∫£\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Ti·ªÅn X·ª≠ l√Ω D·ªØ li·ªáu\n",
        "\n",
        "X·ª≠ l√Ω missing values, outliers, encoding, scaling\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X√°c ƒë·ªãnh target column\n",
        "# THAY ƒê·ªîI THEO DATASET C·ª¶A B·∫†N\n",
        "target_column = \"target\"  # ƒêi·ªÅu ch·ªânh t√™n c·ªôt target\n",
        "\n",
        "if target_column not in df.columns:\n",
        "    print(f\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y c·ªôt '{target_column}'\")\n",
        "    print(f\"C√°c c·ªôt c√≥ s·∫µn: {list(df.columns)}\")\n",
        "    print(\"Vui l√≤ng ch·ªânh s·ª≠a bi·∫øn target_column ·ªü tr√™n\")\n",
        "else:\n",
        "    # T√°ch features v√† target\n",
        "    X = df.drop(columns=[target_column])\n",
        "    y = df[target_column]\n",
        "    \n",
        "    print(f\"Features: {X.shape}\")\n",
        "    print(f\"Target: {y.shape}\")\n",
        "    \n",
        "    # X√°c ƒë·ªãnh lo·∫°i b√†i to√°n\n",
        "    if y.dtype in ['object', 'category'] or y.nunique() < 20:\n",
        "        problem_type = 'classification'\n",
        "        print(\"üìä Lo·∫°i b√†i to√°n: Classification\")\n",
        "    else:\n",
        "        problem_type = 'regression'\n",
        "        print(\"üìä Lo·∫°i b√†i to√°n: Regression\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kh·ªüi t·∫°o preprocessor\n",
        "# ƒêi·ªÅu ch·ªânh c√°c tham s·ªë theo nhu c·∫ßu:\n",
        "# - missing_strategy: 'mean', 'median', 'most_frequent', 'interpolation'\n",
        "# - outlier_method: 'IQR', 'Z-score'\n",
        "# - encoding_method: 'onehot', 'label', 'ordinal'\n",
        "# - scaling_method: 'standard', 'minmax'\n",
        "\n",
        "preprocessor = DataPreprocessor(\n",
        "    missing_strategy='mean',\n",
        "    outlier_method='IQR',\n",
        "    encoding_method='onehot',\n",
        "    scaling_method='standard'\n",
        ")\n",
        "\n",
        "# Fit v√† transform\n",
        "X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "print(f\"‚úÖ ƒê√£ x·ª≠ l√Ω d·ªØ li·ªáu: {X_processed.shape}\")\n",
        "\n",
        "# L∆∞u preprocessor\n",
        "preprocessor.save_preprocessor(\"../src/models/preprocessor.joblib\")\n",
        "print(\"‚úÖ ƒê√£ l∆∞u preprocessor\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chia train/test (80/20)\n",
        "# N·∫øu classification, d√πng stratify\n",
        "stratify = y if problem_type == 'classification' else None\n",
        "\n",
        "X_train, X_test, y_train, y_test = split_data(\n",
        "    X_processed, y, \n",
        "    test_size=0.2, \n",
        "    stratify=stratify,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Train set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "\n",
        "# L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω\n",
        "Path(\"../data/processed\").mkdir(parents=True, exist_ok=True)\n",
        "pd.concat([X_train, y_train], axis=1).to_csv(\"../data/processed/train.csv\", index=False)\n",
        "pd.concat([X_test, y_test], axis=1).to_csv(\"../data/processed/test.csv\", index=False)\n",
        "print(\"‚úÖ ƒê√£ l∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training M√¥ h√¨nh\n",
        "\n",
        "Training 2 m√¥ h√¨nh kh√°c lo·∫°i v·ªõi hyperparameter tuning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model A: Random Forest\n",
        "print(\"=\" * 50)\n",
        "print(\"Training Model A: Random Forest\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "trainer_a = ModelTrainer(\n",
        "    model_type=problem_type,\n",
        "    model_name='random_forest'\n",
        ")\n",
        "\n",
        "trainer_a.train(X_train, y_train, tuning_method='grid', cv=5)\n",
        "print(f\"Best params: {trainer_a.best_params}\")\n",
        "\n",
        "# ƒê√°nh gi√° Model A\n",
        "metrics_a, y_pred_a = trainer_a.evaluate(X_test, y_test)\n",
        "print(f\"\\nMetrics Model A:\")\n",
        "for key, value in metrics_a.items():\n",
        "    print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "# L∆∞u Model A\n",
        "model_a_path, metadata_a_path = trainer_a.save_model(\n",
        "    f\"../src/models/model_a_{problem_type}.joblib\"\n",
        ")\n",
        "print(f\"‚úÖ ƒê√£ l∆∞u Model A: {model_a_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model B: SVM (kh√°c lo·∫°i v·ªõi Model A)\n",
        "print(\"=\" * 50)\n",
        "print(\"Training Model B: SVM\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "trainer_b = ModelTrainer(\n",
        "    model_type=problem_type,\n",
        "    model_name='svm'\n",
        ")\n",
        "\n",
        "trainer_b.train(X_train, y_train, tuning_method='random', cv=5, n_iter=20)\n",
        "print(f\"Best params: {trainer_b.best_params}\")\n",
        "\n",
        "# ƒê√°nh gi√° Model B\n",
        "metrics_b, y_pred_b = trainer_b.evaluate(X_test, y_test)\n",
        "print(f\"\\nMetrics Model B:\")\n",
        "for key, value in metrics_b.items():\n",
        "    print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "# L∆∞u Model B\n",
        "model_b_path, metadata_b_path = trainer_b.save_model(\n",
        "    f\"../src/models/model_b_{problem_type}.joblib\"\n",
        ")\n",
        "print(f\"‚úÖ ƒê√£ l∆∞u Model B: {model_b_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. ƒê√°nh gi√° v√† So s√°nh M√¥ h√¨nh\n",
        "\n",
        "T√≠nh metrics v√† t·∫°o visualizations\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ƒê√°nh gi√° Model A\n",
        "evaluator_a = ModelEvaluator(model_type=problem_type)\n",
        "\n",
        "if problem_type == 'classification':\n",
        "    metrics_a, fig_a = evaluator_a.evaluate_classification(\n",
        "        y_test, y_pred_a, \n",
        "        save_path=\"../src/evaluation/model_a_evaluation.png\"\n",
        "    )\n",
        "elif problem_type == 'regression':\n",
        "    metrics_a, fig_a = evaluator_a.evaluate_regression(\n",
        "        y_test, y_pred_a,\n",
        "        save_path=\"../src/evaluation/model_a_evaluation.png\"\n",
        "    )\n",
        "\n",
        "evaluator_a.save_results(\"../src/evaluation/model_a_results.json\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ƒê√°nh gi√° Model B\n",
        "evaluator_b = ModelEvaluator(model_type=problem_type)\n",
        "\n",
        "if problem_type == 'classification':\n",
        "    metrics_b, fig_b = evaluator_b.evaluate_classification(\n",
        "        y_test, y_pred_b,\n",
        "        save_path=\"../src/evaluation/model_b_evaluation.png\"\n",
        "    )\n",
        "elif problem_type == 'regression':\n",
        "    metrics_b, fig_b = evaluator_b.evaluate_regression(\n",
        "        y_test, y_pred_b,\n",
        "        save_path=\"../src/evaluation/model_b_evaluation.png\"\n",
        "    )\n",
        "\n",
        "evaluator_b.save_results(\"../src/evaluation/model_b_results.json\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# So s√°nh 2 m√¥ h√¨nh\n",
        "comparison_results = {\n",
        "    'Model A (Random Forest)': metrics_a,\n",
        "    'Model B (SVM)': metrics_b\n",
        "}\n",
        "\n",
        "comparison_df, fig_compare = evaluator_a.compare_models(\n",
        "    comparison_results,\n",
        "    save_path=\"../src/evaluation/model_comparison.png\"\n",
        ")\n",
        "\n",
        "print(\"\\nüìä So s√°nh M√¥ h√¨nh:\")\n",
        "print(comparison_df)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. T·∫°o B√°o c√°o ƒê√°nh gi√°\n",
        "\n",
        "L∆∞u b√°o c√°o ƒë√°nh gi√° v√†o report/evaluation.md\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# T·∫°o b√°o c√°o ƒë√°nh gi√°\n",
        "report_path = \"../report/evaluation.md\"\n",
        "Path(report_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with open(report_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(\"# B√°o c√°o ƒê√°nh gi√° M√¥ h√¨nh\\n\\n\")\n",
        "    f.write(f\"## Lo·∫°i b√†i to√°n: {problem_type.upper()}\\n\\n\")\n",
        "    \n",
        "    f.write(\"## Model A: Random Forest\\n\\n\")\n",
        "    f.write(\"### Metrics:\\n\")\n",
        "    for key, value in metrics_a.items():\n",
        "        f.write(f\"- **{key}**: {value:.4f}\\n\")\n",
        "    f.write(\"\\n\")\n",
        "    \n",
        "    f.write(\"## Model B: SVM\\n\\n\")\n",
        "    f.write(\"### Metrics:\\n\")\n",
        "    for key, value in metrics_b.items():\n",
        "        f.write(f\"- **{key}**: {value:.4f}\\n\")\n",
        "    f.write(\"\\n\")\n",
        "    \n",
        "    f.write(\"## So s√°nh\\n\\n\")\n",
        "    f.write(\"| Metric | Model A | Model B |\\n\")\n",
        "    f.write(\"|--------|---------|----------|\\n\")\n",
        "    for metric in metrics_a.keys():\n",
        "        f.write(f\"| {metric} | {metrics_a[metric]:.4f} | {metrics_b[metric]:.4f} |\\n\")\n",
        "    f.write(\"\\n\")\n",
        "    \n",
        "    # X√°c ƒë·ªãnh m√¥ h√¨nh t·ªët h∆°n\n",
        "    if problem_type == 'classification':\n",
        "        best_metric = 'accuracy'\n",
        "    else:\n",
        "        best_metric = 'r2' if 'r2' in metrics_a else 'rmse'\n",
        "    \n",
        "    if best_metric in metrics_a and best_metric in metrics_b:\n",
        "        if problem_type == 'regression' and best_metric == 'rmse':\n",
        "            best_model = 'A' if metrics_a[best_metric] < metrics_b[best_metric] else 'B'\n",
        "        else:\n",
        "            best_model = 'A' if metrics_a[best_metric] > metrics_b[best_metric] else 'B'\n",
        "        f.write(f\"## K·∫øt lu·∫≠n\\n\\n\")\n",
        "        f.write(f\"M√¥ h√¨nh t·ªët h∆°n d·ª±a tr√™n {best_metric}: **Model {best_model}**\\n\")\n",
        "\n",
        "print(f\"‚úÖ ƒê√£ t·∫°o b√°o c√°o: {report_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ho√†n th√†nh!\n",
        "\n",
        "ƒê√£ th·ª±c hi·ªán xong pipeline:\n",
        "- ‚úÖ Ki·ªÉm tra v√† th·ªëng k√™ dataset\n",
        "- ‚úÖ Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu\n",
        "- ‚úÖ Training 2 m√¥ h√¨nh kh√°c lo·∫°i\n",
        "- ‚úÖ ƒê√°nh gi√° v√† so s√°nh m√¥ h√¨nh\n",
        "- ‚úÖ L∆∞u k·∫øt qu·∫£\n",
        "\n",
        "**C√°c file ƒë√£ t·∫°o:**\n",
        "- `report/descriptive_stats.md` - Th·ªëng k√™ m√¥ t·∫£\n",
        "- `src/models/preprocessor.joblib` - Preprocessor\n",
        "- `src/models/model_a_*.joblib` - Model A\n",
        "- `src/models/model_b_*.joblib` - Model B\n",
        "- `src/evaluation/*.png` - H√¨nh ·∫£nh ƒë√°nh gi√°\n",
        "- `src/evaluation/*.json` - K·∫øt qu·∫£ metrics\n",
        "- `report/evaluation.md` - B√°o c√°o ƒë√°nh gi√°\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
